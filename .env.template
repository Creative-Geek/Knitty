# Fast LLM Configuration (for keyword extraction)
# Any LLM larger than 14B is recommended
FAST_LLM_API_KEY="your-fast-llm-api-key-here"
FAST_LLM_API_BASE="https://api.groq.com/openai/v1"
FAST_LLM_MODEL_NAME="openai/gpt-oss-120b"

# Context LLM Configuration (for large document processing)
# Gemini 2.5 Flash is recommended
CONTEXT_LLM_API_KEY="your-context-llm-api-key-here"
CONTEXT_LLM_API_BASE="https://generativelanguage.googleapis.com/v1beta/openai/"
CONTEXT_LLM_MODEL_NAME="gemini-2.5-flash"

# Embed LLM Configuration (for generating embeddings)
# Local Embedding using embeddinggemma is recommended
EMBED_LLM_API_KEY="your-embed-llm-api-key-here"
EMBED_LLM_API_BASE="http://localhost:11434"
EMBED_LLM_MODEL_NAME="embeddinggemma:latest"


# Smart LLM (for CV generation)
# Gemini 2.5 Pro is recommended
SMART_LLM_API_KEY="your-smart-llm-api-key-here"
SMART_LLM_API_BASE="https://generativelanguage.googleapis.com/v1beta/openai"
SMART_LLM_MODEL_NAME="gemini-2.5-pro"

# if you have access to cv-factory-api
SPECIAL_SAUCE_API_KEY="your-special-sauce-api-key-here"
SPECIAL_SAUCE_API_URL="your-special-sauce-api-url-here"
